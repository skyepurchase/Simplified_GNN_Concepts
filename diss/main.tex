\documentclass[12pt,a4paper,openany,openright]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=20mm]{geometry}
\usepackage[pdfborder={0 0 0},backref=page]{hyperref}
\usepackage{dissertation}

\include{math_commands}

\begin{document}

\include{metadata}

\bibliographystyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\thispagestyle{empty}

\rightline{\LARGE \textbf{\mfullname}}

\vspace*{60mm}
\begin{center}
    \Huge
    \textbf{\mtitle} \\[5mm]
    \mexamination \\[5mm]
    \mcollege \\[5mm]
    \mdate
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma

\pagestyle{plain}

\newpage
\newpage
\section*{Declaration of originality}

I, \mfullname{} of \mcollege, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose. \mconsent

\bigskip
\leftline{Signed \msignature}
\bigskip
\leftline{Date \today}

\chapter*{Proforma}

{\large
\begin{tabular}{p{0.3\linewidth}p{0.6\linewidth}}
    Candidate Number:   & \bf \mcandidate                   \\
    Project Title:      & \bf \mtitle                       \\
    Examination:        & \bf \mexamination, 2023           \\
    Word Count:         & \bf \mwordcount\footnotemark[1]   \\
    Code Line Count:    & \bf \mlinecount\footnotemark[2]   \\
    Project Originator: & \moriginator                      \\
    Supervisor:         & \msupervisor                      \\ 
\end{tabular}
}

\footnotetext[1]{This word count was computed by \texttt{texcount} using the options \texttt{\%group table 0 1} and \texttt{\%group tabular 1 1} to count tables.}
\footnotetext[2]{This word count was computed by \texttt{cloc}. Only generating bash scripts where considered as the generated files are all structurally identical.}
\stepcounter{footnote}

\section*{Original Aims of the Project}

\error{Include why the reader should care}
This project analyses the effect that simplifying graph neural networks (GNNs), through linearisation, has on model performance and why this is the case.
%the graph concepts that are extracted from a trained model.
The project focuses on the linear GNN, Simplified Graph Convolution \cite{wu2019simplifying}, as it has been shown to match the performance of traditional, non-linear GNNs on a sample of benchmark datasets.
%which removes the non-linearity between layers from previous graph neural network approaches reducing the problem of graph representational learning to a precomputation on the graph structure and a single linear regressor.
Using the notion of graph concepts proposed in \textit{Magister et. al} \cite{magister2021gcexplainer} the project aims to compare SGC to Graph Convolution Network to determine the effect of linearising GNNs.
The overall aim of the project is to better understand how linear architectures can achieve graph structure awareness.
These insights then guide extensions to improve linear GNN architectures and reduce computational cost.
%Further extensions then build on SGC to see which techniques can improve accuracy and concept metrics.

\section*{Work Completed}

The project was a success gaining insight into the graph structure awareness of both linear and non-linear models.
The insight gained lead to two new approaches to graph learning
\begin{enumerate}
    \item Using both linear and non-linear GNNs in one model to gain the benefit of both approaches.
    \item A new linear GNN which aggregates linear graph filters to improve graph structure awareness.
\end{enumerate}
The project further demonstrated that the SGC architecture proposed by \textit{Wu et al.} does not have fine-grained graph structure awareness and demonstrates poor performance on a range of new datasets.
%Furthermore, it demonstrated that in the case of highly synthetic data SGC is unable to match the performance of GCN by a significant margin.
%My extensions demonstrate that in the case of real-world graph datasets, where graph structure is important, SGC continues to underperform compared to GCN and does not produce comparable concepts.
%To combat this a novel extension to SGC using jumping knowledge networks\cite{xu2018representation} is presented that demonstrates graph structure awareness in a linear model.
%However, overall SGC does not appear to be a suitable candidate for graph representational learning on its own.
%I additionally set out to create a new parameterised dataset to further analyse the shortcomings of SGC when dealing with graph structure.
%\error{This is still not complete but I am undertaking this task.}

\section*{Special Difficulties}

None

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapters

\pagestyle{headings}

\tableofcontents

\input{chapters/introduction}
\input{chapters/preparation}
\input{chapters/implementation}
\input{chapters/evaluation}
\input{chapters/conclusion}

\bibliography{references}

\appendix
\input{appendices/abbreviations}
\input{appendices/hyperparameters}
\input{appendices/concepts}
\input{appendices/proof}
\input{appendices/gcn}
\input{appendices/datasets}
\input{appendices/phase2}

\end{document}
