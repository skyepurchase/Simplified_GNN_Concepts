\documentclass{article}
\usepackage{log_2022}            % for camera-ready version
% \usepackage[review]{log_2022}  % for anonymous submission
% \usepackage[preprint]{log_2022}
%                                % for preprint version
% \usepackage[eatrack]{log_2022}
%                                % for accepted extended abstracts

\usepackage{booktabs}            % professional-quality tables
\usepackage{multirow}            % tabular cells spanning multiple rows
\usepackage{amsfonts}            % blackboard math symbols
\usepackage{graphicx}            % figures
\usepackage{duckuments}          % sample images

% If you want to use natbib:
\usepackage[numbers,compress,sort]{natbib}
%                                % for numerical citations
% \usepackage[sort,round]{natbib}
%                                % for textual citations

% If you want to use bibLaTeX, uncomment statements below:
% \usepackage[
%      backend=biber,
%      style=numeric-comp,
%      backref=true,
%      natbib=true]{biblatex}
% \addbibresource{reference.bib}

\newcommand\note[1]{\textcolor{blue}{\textsc{\textbf{[NB:} #1\textbf{]}}}}
\newcommand\error[1]{\textcolor{red}{\textsc{\textbf{[Error: #1]}}}}

\title[Extracting Concepts from Simplified Graph Neural Networks]{Extracting Concepts from Simplified Graph Neural Networks}

\author[S. Purchase et al.]{%
Skye Purchase\\
\institute{University of Cambridge}\\
\email{skyepurchase@gmail.com}\And
Lucy Charlotte Magister\\
\institute{University of Cambridge}\\
\email{Fillin}\And
Pietro Lio\\
\institute{University of Cambridge}\\
\email{Fillin}
}

\begin{document}

\bibliographystyle{unsrtnat}

\maketitle

\begin{abstract}
    \error{Complete once the direction of the paper is decided.}
% Short introduction giving a full overview

    This paper explores the effect that simplifying graph neural network (GNN) architectures through linearization has on performance and graph structure awareness.
    It demonstrates that the current approach to linearising GNNs results in poor graph structure awareness. In response, I present two novel approaches to graph representational learning in response.
    The first approach splices together elements of linear and non-linear GNNs into a single model and the second demonstrates accurate graph structure awareness whilst remaining quasi-linear.
%Specifically it focuses on the ideas of graph concepts which are extracted from the trained model to provide a visual demonstration of which subspaces of the input influence the model's choice of label.
%These concepts are compared to the original Graph Convolution Network (GCN~\cite{kipf2016semi}) using the metrics of concept purity and completeness proposed in \citet{magister2021gcexplainer}.
%The specific linear model chosen is the Simplified Graph Convolution (SGC) proposed in \citet{wu2019simplifying} due to claims that it matches the performance of GCN~\cite{kipf2016semi}.
%Further studies are conducted extending the basic architecture of SGC, whilst keeping with the theme of simplified graph neural networks, to see the effect on the accuracy and concept scores.

\end{abstract}

\input{sections/introduction}
\input{sections/related_work}

\bibliography{references}


\end{document}
