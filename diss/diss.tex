\documentclass[12pt,a4paper,openany,openright]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=20mm]{geometry}
\usepackage[pdfborder={0 0 0},backref=page]{hyperref}
\usepackage{dissertation}

\include{math_commands}

\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=black,
    citecolor=black}

\begin{document}
\pagenumbering{roman}

\include{metadata}

\bibliographystyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\thispagestyle{empty}

\rightline{\LARGE \textbf{\mfullname}}

\vspace*{60mm}
\begin{center}
    \Huge
    \textbf{\mtitle} \\[5mm]
    \mexamination \\[5mm]
    \mcollege \\[5mm]
    \mdate
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma

\pagestyle{plain}

\newpage
\newpage
\section*{Declaration of originality}

I, \mfullname{} of \mcollege, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose. In preparation of this dissertation I did not use text from AI-assisted platforms generating natural language answers to user queries, including but not limited to ChatGPT. \mconsent

\bigskip
\leftline{Signed \msignature}
\bigskip
\leftline{Date \today}

\chapter*{Proforma}

{\large
\begin{tabular}{p{0.3\linewidth}p{0.6\linewidth}}
    Candidate Number:   & \bf \mcandidate                   \\
    Project Title:      & \bf \mtitle                       \\
    Examination:        & \bf \mexamination, 2023           \\
    Word Count:         & \bf \mwordcount\footnotemark[1]   \\
    Code Line Count:    & \bf \mlinecount\footnotemark[2]   \\
    Project Originator: & \moriginator                      \\
    Supervisor:         & \msupervisor                      \\ 
\end{tabular}
}

\footnotetext[1]{This word count was computed by \texttt{texcount} using the options \texttt{\%group table 0 1} and \texttt{\%group tabular 1 1} to count tables.}
\footnotetext[2]{This word count was computed by \texttt{cloc}. Only generating bash scripts where considered as the generated files are all structurally identical.}
\stepcounter{footnote}

\section*{Original Aims of the Project}

As highly-connected data becomes more prevalent \emph{graph neural networks}(GNNs) are becoming more important for real-world applications. 
Understanding how they work and how to reduce the cost of training is therefore important.
This project works towards this goal by analysing how simplifying GNNs effects model performance and graph structure awareness.
The analysis compares the linear \emph{simplified graph convolution}(SGC)~\cite{wu2019simplifying} to the non-linear \emph{graph convolution network}(GCN)~\cite{kipf2016semi} using the explainability framework proposed by \textit{Magister et. al}~\cite{magister2021gcexplainer}.
The insights gained guide extensions to improve the accuracy of linear GNN architectures whilst maintaining the reduced computational cost due to linearisation.
%Further extensions then build on SGC to see which techniques can improve accuracy and concept metrics.

\section*{Work Completed}

The project was a success in gaining insight into the graph structure awareness of both linear and non-linear models.
The insight gained led to two new approaches to graph learning:

\begin{enumerate}[nolistsep]
    \item Splicing together elements of two GNNs to gain the benefit of both architectures. In this case using a linear and non-linear GNN to produce an efficient and high-accuracy model.
    \item A linear GNN which aggregates the output of successive graph filters achieving accurate graph structure awareness.
\end{enumerate}

The project further demonstrated that the SGC architecture proposed by \textit{Wu et al.}~\cite{wu2019simplifying} does not have fine-grained graph structure awareness and demonstrates poor performance on a range of new datasets.
%Furthermore, it demonstrated that in the case of highly synthetic data SGC is unable to match the performance of GCN by a significant margin.
%My extensions demonstrate that in the case of real-world graph datasets, where graph structure is important, SGC continues to underperform compared to GCN and does not produce comparable concepts.
%To combat this a novel extension to SGC using jumping knowledge networks~\cite{xu2018representation} is presented that demonstrates graph structure awareness in a linear model.
%However, overall SGC does not appear to be a suitable candidate for graph representational learning on its own.
%I additionally set out to create a new parameterised dataset to further analyse the shortcomings of SGC when dealing with graph structure.
%\error{This is still not complete but I am undertaking this task.}

\section*{Special Difficulties}

None

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapters

\pagestyle{headings}

\tableofcontents
\clearpage
\pagenumbering{arabic}

\input{chapters/introduction}
\input{chapters/preparation}
\input{chapters/implementation}
\input{chapters/evaluation}
\input{chapters/conclusion}

\bibliography{references}

\appendix
\input{appendices/abbreviations}
\input{appendices/hyperparameters}
\input{appendices/concepts}
\input{appendices/proof}
\input{appendices/gcn}
\input{appendices/datasets}
\input{appendices/phase2}

\end{document}
