\chapter{Implementation}

\section{Datasets}
\label{sec:datasets-imp}
\Sref{sec:datasets-theory} presents a number of datasets that cover a range of different GNN training styles and motifs.
The inductive datasets discussed in \Sref{sec:RWD} are already packaged and available through \texttt{PyTorch Geometric}\cite{Fey/Lenssen/2019} as well as the Planetoid\cite{planetoid}\cite{citation} datasets.

\paragraph{Synthetic datasets}
The synthetic datasets discussed in \Sref{sec:synth} are proposed by \textit{Ying et al.}\cite{ying2019gnnexplainer} are available pre-built in their github and \texttt{PyTorch Geometric} provide limited versions of a small subset I decide to implement them myself.
This allows for more rigorous testing 
%of both the datasets and the SGC precomputation later 
described in \Sref{sec:testing-imp}.

The motifs described in \Sref{sec:synth} can be hard-coded.
%each motif is therefore an adjacency matrix (in coordinate form) and a label vector.
When generating the dataset the base graph adjacency is generated first. 
%with a label vector assigning every node to class 0.
%The Barabasi-Albert graph is generated probabilistically adding one node at a time and connecting each node to the existing graph based on the degree of the nodes currently in the graph.
Then each motif required is 
%the hard-coded tensors are adjusted and 
appended to the on going adjacency matrix and label vector by connecting
%This adjustment includes adding an additional bidirectional connection between 
a random node in the base graph to a predetermined node of the motif.

The datasets need to be compliant with \texttt{PyTorch Geometric} to be used correctly during training.
\texttt{PyTorch Geometric} provides an \texttt{InMemoryDataset} abstract class for datasets that are small enough to be generated and stored within RAM.
%As these are transductive datasets as described in \Sref{sec:datasets-theory} 
Each dataset contains a single graph \texttt{Data} object is created that is then accessed through a dataloader.

The \texttt{Data} object includes train and test split masks randomly assigning nodes to achieve an 80:20 ratio based on seed value.
%This random assignment is based on the seed value for the experiment discussed in \Sref{sec:reproducibility}.
This 
%randomness also applies to the generation of the graph 
creates significantly different graphs each generation
meaning that nodes for one seed value are effectively unseen for another seed value.

\paragraph{SGC}
The majority of SGC is the pre-computation
\begin{equation}
    \label{eq:pre-comp}
    \mS^k\mX
\end{equation}
of equation \ref{eq:SGC} where $\mS$ is the filter defined in equations \ref{eq:op} and \ref{eq:norm}.
As $\mS$ 
%is defined in terms of the adjacency matrix which 
does not change during training equation \ref{eq:pre-comp} can be applied before training.
This process is achieved with three functions that calculate the normalised filter $\mS$,
%by computing the required matrices and multiplications, 
convert the matrix form and successively apply the filter to the node representations.

This results in a new feature matrix which replaces the \texttt{Data} object features.
Thus the SGC model does not need any adjacency matrix and can instead be implemented as a classifier.
For the sake of consistency and to aid the concept evaluation functions in \Sref{sec:concepts} the adjacency matrix is keep.
%A new \texttt{Data} object is created copying the labels, data splits and adjacency of the input graph but with the pre-computation matrix as the feature matrix.
The \texttt{Data} object is copied with updated features allowing for the implementation of the SGC and GCN mixed model proposed in \Sref{sec:SGCN}.

\section{Models}
\label{sec:models}

\texttt{PyTorch Geometric} provides an abstract \texttt{MessagePassing} class which defines a generic GNN layer.
Beyond the standard \texttt{forward} method of \texttt{PyTorch}\cite{paszke2019pytorch} an addition \texttt{message\_passing} method is also present.
This method is called during the \texttt{forward} method providing the node representations and associated neighbourhood nodes.
This method allows the implementation of the function
\begin{equation}
    \mH^{(l+1)} = \mS\mH^{(l)}\bm{\Theta}^{(l)},
\end{equation}
described in \ref{sec:GCN} as equation \ref{eq:GCN1}.
During the \texttt{message\_passing} method the normalised adjacency $\mS$ is applied to the node representations $\mH^{(l)}$.
Then a linear layer is applied to the resulting representations in the \texttt{forward} method.

These GCN layers can then be combined with ReLU non-linearity to form a GCN model.
The GCN model additionally includes a single linear layer for classification.
In comparison SGC needs only to be a classifier as explained in \Sref{sec:datasets-imp} and so contains a single linear layer.

\section{Machine Learning Pipeline}
\label{sec:pipeline}

The ML pipeline is the infrastructure that links the datasets, pre-computation, dataloaders, and models together to train and evaluate the models.
This allows for specific experiment configurations to be run changing each component individually or together.
As the main aspect of this project is concept extraction, evaluation, and interpretation I use \texttt{PyTorch Lightning} as the main component of the pipeline.

\texttt{PyTorch Lightning} provides \texttt{Lightning Modules} that act as wrappers around standard \texttt{PyTorch} (and therefore \texttt{PyTorch Geometric}) modules and carry out all the required training, validation and testing loops.
Unfortunately, for the purposes of the core project and extensions a single wrapper is not possible.
This is because the wrappers need to behave differently for SGC and GCN as GCN needs additional adjacency information.
Additionally the real-world graph classification datasets require one-hot encoding when calculating loss whereas the synthetic datasets do not.
Therefore the project implements 5 different wrappers to accomodate these differences.

The datasets, pre-computation and dataloaders discussed in \Sref{sec:datasets-imp}, the models described in \Sref{sec:models} and the wrappers are then instantiated, run and results saved in a single \texttt{main.py} file.
To run a single experiment the specific model build, dataset, seed, save destinations, etc. need to be passed to this function.
To reduce this overhead multiple bash scripts can be dynamically created to run single quick experiments or full experiments which test the model on prechosen random seeds.

\subsection{Reproducibility}
\label{sec:reproducibility}
It is important that the results that are achieved by the models, especially in regards to model accuracy, can be replicated later.
This is both for the results presented in this dissertation but also for results found during experimentation in the extension phase.

To achieve this every single experiment must set a seed value which is saved along with the results.
This way when running the same configuration again using the same seed the expected outcome should be the same within very small bounds.

Furthermore, the results achieved should be linked to specific model build that identifies all the hyperparameters used.
The model build specifies every aspect of the model so that the exact same model can be replicated in a completely different ML pipeline.
Thus the build files need to be human readable and easily expandable.

To achieve these two requirements I use YAML which mimics python in its syntax and is therefore very readable but also links well with the rest of the pipeline.
Each build has a unique YAML file with the filename following the convention \texttt{<model>.<dataloader>.<dataset>.<version>.yml}.
Storing the YAML filename, seed and timestamp with every set of results allows for the reproduction of almost every result that was achieved throughout the project.

\subsection{Experimentation}
The infrastructure used in \Sref{sec:reproducibility} also add in experimentation but linking interesting or desired results to the specific configuration that produced them.
This is particularly desirable during the extension phase where the exact method that will produce the best results is unknown.

Beyond quick iteration in models during the exploratory extensions finding the optimal hyperparameters is essential.
Incorrect hyperparameters can lead to artificially low performance and thus invalid the interpretation of the models based on results.
I therefore implement \texttt{sweep.py} which takes a set of different hyperparameters values to test for each hyperparameter and runs a short evaluation of the performance.
To properly evaluate models it is important that this evaluation does not use nodes that are in the final test set.
As discussed in \Sref{sec:datasets-imp} choosing different seeds to the final run effectively means that nodes in the prechosen random seeds test set are always unseen.

\section{Concept Extraction and Evaluation}
\label{sec:concepts}

\subsection{Extraction}
\label{sec:extraction}

This subsection uses idea of \emph{activation spaces} which are multidimensional spaces within which the node representations before or after a layer are expressed.
Therefore there are two stages to concept extraction as discussed in \Sref{sec:GCE}: storing the activation space and clustering the activation space
The storing of the activations occurs during the experiment run whereas the clustering occurs afterwards to allow for quicker tests on the activation space.

\paragraph{storing}
SGC and GCN behave different in regards to storing activation spaces as in SGC the node representations are pre-computed for each layer whereas GCN calculates this during training.

The simpler case is SGC where the node representations from each successive filter application can be stored in a dictionary and written out to a file.
This process allows for multiple layers to be analysed without having to train the SGC model, this in turn can be used to optimise SGC before training though this is not explored.

In comparison GCN requires an additional function during the forward pass of the model to save the node representations to a permanent store.
\texttt{PyTorch}\cite{paszke2019pytorch} provides this functionality through \emph{hooks} which are functions that can be attached to modules within a model.
This function will then run whenever the modules \texttt{forward} method is run and \texttt{PyTorch} will pass the input, output and module to the function.
Thus a simple hook function is created which updates saves the modules output (which are the node representations) to a dictionary of module identifier, activation pairs.
This dictionary is written out to a file after testing is complete this means that the activations represent the final model.

\paragraph{clustering}
The described activation dictionaries can be loaded and for each of the layers the node representations are clustered using k-means.
K-means is chosen for clustering as this is the best performing clustering approach as demonstrated in \textit{Magister et al.}\cite{magister2021gcexplainer}.
Rather than implement my own k-means algorithm I utilise the algorithm already present within \texttt{scikit learn}\cite{scikit-learn}.

The clustering is parameterised by a tunable number of clusters.
The concept scores in \Sref{sec:GCE} can then motivate the choice of this value though focusing on maximising the completeness and purity will not lead necessarily to better concepts.

The receptive field of the nodes then needs to be determined as well.
This determines for each node of interest the size of the receptive field, that is how many hops between connected nodes are allowed.
This is also controlled by a tunable number of hops and the choice of which is guided by the purity of the concepts.

To compare SGC and GCN the same concept parameters presented in \textit{GCExplainer}\cite{magister2021gcexplainer} are used.

\subsection{Evaluation}
\label{sec:concept-eval}

Once the concepts are extracted using the methods detailed in \Sref{sec:extraction} the two concept metrics can be calculated.
The concepts from a model are extracted from each layer using the same clustering and receptive field constants provided.
The theory behind the two concept metrics of completeness and purity are outlined in \Sref{sec:GCE}.

\paragraph{Completeness}
As discussed in \Sref{sec:GCE} completeness is determined by whether the extracted concepts can be used to infer the resulting class of a node.
This is achieved by fitting a decision tree\cite{kazhdan2020now} as proposed by \textit{Magister et al.}\cite{magister2021gcexplainer} to the concept data and labels.
the accuracy of this decision tree on a held out test set then represents the completeness of the concepts extracted.

\note{I think this needs rewriting.}
The decision tree needs to predict a label for each of the nodes of the graph based on the concept the node belongs to.
The input data to the decision tree is a vector of predicted clusters for each of the nodes based on the k-means clustering.
The output vector is then constructed by matching the node class to the node identifier for each node in the input vector.

\paragraph{Purity}
To calculate the purity the three quintessential subgraphs of each concept are chosen.
Each quintessential subgraph centred on a node of interest chosen based on distance to its centroid.
These subgraphs are constructed through a breadth-first search through the input graph starting from the node of interest.

The purity is then calculated by taking the average of the GED described in \Sref{sec:GCE} between these concepts.
The graph edits that are allowed are to add or remove a node or edge noting that the features of the nodes is not considered.
This provides a similarity of the concepts based on graph structure rather than the representations of the nodes.

The reason for only the three quintessential graphs to be considered is due to the computational cost of calculating GED.
In some cases, where the number of nodes in a subgraph exceeds 13, the purity calculation is skipped.
It is possible to determine an upperbound on GED in polynomial time however this would negatively impact the purity scores of Barabasi-Albert concepts.
This in turn would make models that use more Barabasi-Albert concepts appear to be less pure when the concepts are in fact very coherent.

\subsection{Visualisation}
\label{sec:vis}
The visualisation of the concepts uses the same approach as the calculation of purity in \Sref{sec:concept-eval}.
However, more quintessential graphs may be considered to better visualise the extracted concepts.
This does mean that the visual results may appear contradictory to the purity score as subgraphs further from the centroid may not match the purity reported.

For the synthetic graphs the subgraphs are coloured with two colours, green indicating the node of interest and pink indicating neighbouring nodes.
In some instances on concept may contain a node of interest of an other visualised concept, in this case the subgraph is visualised with mutliple green nodes.
This represents the fact that the model does not distinguish between these nodes within the graph structure.
This process is also used for REDDIT-BINARY\cite{Morris+2020}.

In the case of the Mutagenicity\cite{Morris+2020} dataset there already exist standard chemical colour schemes for atoms.
\textit{Magister et al.}\cite{magister2021gcexplainer} assign a unique colour to each atom within a concept which means that between concepts the colour scheme does not match.
Instead, I define pre-determined values for each atom based on a chemical colour scheme giving a better comparison across concepts.

In all cases each subgraph is titled by the label of the node of interest.
This provides additional information about the coherence of the concepts which purity does consider which is whether all the nodes have the same label.

\section{Testing}
\label{sec:testing-imp}
As discussed in \Sref{sec:testing} the project utilises two forms of testing to verify the implementation.
On top of these I regularly run the pipeline described in \Sref{sec:pipeline}, this verifies that the modules being developed integrate correctly.
Where modules are not implemented dummy data or functions are used instead.

\paragraph{Unit testing}
Three components of the software follow traditional software development and therefore unit tests are used to verify implementation
\begin{itemize}
    \item[] 
        \smalltitle{Synthetic datasets} 
        These have strict defining properties based on the parameters passed during initialisation.
        Verification of the datasets can be done with hard-coded tensors for small initialisations to verify the correct motif is used or base graph.
        For larger instances of the graphs more general properties such as the expected number of nodes, number of classes, and number of nodes within a given class can be used to verify that the correct procedure was taken.
    \item[] 
        \smalltitle{Pre-computation}
        This is a theoretical calculation where the specific matrix multiplications have already been verified through the derivation in \Sref{sec:GCN} and \Sref{sec:SGC}.
        However, to verify that these are correctly implemented and that the application of the filter is correct small test graphs are generated.
        In these cases the expected result can be calculated by hand or external matrix calculators.
    \item[] 
        \smalltitle{Concept evaluation}
        Though parts of this implementation use algorithms provided by \texttt{scikit learn}\cite{scikit-learn} the results of these functions needs to be verified in the new context.
        By providing small graphs with precise node representations specific clusters are expected and can be verified by comparing the predicted clusters to the expected clusterings.
        In the case of purity the motifs themselves can be passed to verify that they are completely pure, as well as isomorphic graphs and non-isomorphic of known GED.
\end{itemize}

\paragraph{Reproduction of prior results}
Successful reproduction of both \textit{Magister et al.}\cite{magister2021gcexplainer}'s and \textit{Wu et al.}\cite{wu2019simplifying}'s results in presented in \Sref{sec:reproduction}.

\section{Extensions}
\label{sec:extensions-imp}
After completing the core project my extensions focus on improving the low performance demonstrated by SGC in \Sref{sec:comp-acc}.
The motivation for some of my extensions develops from the results and analysis of the core project however the implementation details are non-trivial.
This section outlines the three extensions completed, the motivation for each and the implementation details.
The evaluation of my extensions is presented in \Sref{sec:extension-eval}.

\subsection{SGC graph classification}
\paragraph{Motivation}
The datasets present in \textit{Magister et al.}\cite{magister2021gcexplainer} include 2 real world datasets that focus on graph classification.
As discussed in \Sref{sec:datasets-theory} these graph classification datasets are also inductive rather than transductive which provides another test of the capabilities of SGC.
Furthermore, though \Sref{sec:comp-acc} suggests SGC performs poorly, this could be because of the synthetic nature of the datasets.

\paragraph{Prior work}
\textit{Wu et al.}\cite{wu2019simplifying} discuss graph classification for SGC suggesting that it can substitute GCN in a deep graph convolutional neural network as proposed by \textit{Zhang et al.}\cite{zhang2018end}.
However, \textit{Magister et al.} carry out graph classification using GCN in a different way.
Instead they utilise pooling on the graph node representations from a GCN on each graph in the dataset.
The label of the entire graph can then be inferred from this single representation.
As this latter method is easier to implement and allows for a fairer comparison between SGC and the results achieved by \textit{Magister et al.} this approach is chosen.
The resulting model is identical to the standard SGC model with addition of a pooling layer before the classifier.

\paragraph{Implementation}
The only problem posed is concept extraction as this is done on the node level, as suggested by \textit{Magister et al.}, rather than the graph level.
Each graph has a single label with no labels for the node, therefore the method proposed in \Sref{sec:concept-eval}, does not work.
This is overcome by broadcasting the graph label to each of the nodes in the graph.
To achieve this additional information about the batch identifier of each node and each label is used to match labels to nodes.

This allows the graphs to be combined into a disconnected forest of graphs and clustering can be carried out on this forest.
Calculation of concept scores and the visualisation of concept therefore remains the same as that presented in \Sref{sec:concept-eval} and \Sref{sec:vis}.

\subsection{SGC and GCN mixed model}
\label{sec:SGCN}
\paragraph{Motivation}
The derivation in \Sref{sec:SGC} shows that SGC is directly derived from GCN using the same graph filter.
Therefore, though the two models use this filter differently they should be interchangeable to some extent.

Futhermore the benefit of SGC is to reduce the complexity of GCN and the cost of training by pre-computing the graph operation.
Even though for the very small datasets in \Sref{sec:datasets} the improvement on cost and reduction in parameters is not significant larger datasets may create a larger benefit.

However, due to the low accuracy of SGC seen in \Sref{sec:comp-acc} some properties of GCN must required for high accuracy 
A mixture of both should therefore yield a high accuracy model with fast pre-computation.
This model will be referred to as SGCN.

\paragraph{Implementation}
For simplicity and to utilise the power of pre-computation SGCN starts with SGC layers and then transitions to GCN layers.
For a fairer comparison the total number of layers must remain the same as the corresponding SGC and GCN layers.
This reduces the problem to finding where SGC and GCN agree the most in regards to their concepts.

This is achieved using \emph{adjusted mutual information}(AMI) between the two models by using the probability of a node appearing in a specific cluster from either model.
This gives a measure for the dependence of the models clusters and therefore how similar the models are.
However, this does not take into account the random chance of two nodes appearing in the same cluster.
Therefore the mutual information is adjusted for this chance resulting in a number in the range $[0, 1]$ where $1$ is identical.
This measure limits the artificial inflation of mutual information based on increasing the number of clusters.

To better visualise these results the dimensionality of the activations from the models are reduced using \emph{t-distributed stochastic neighbor embedding} (t-SNE) into 2 dimensions.
This clusters similar representations together and keeps different representations apart which acts as a visual proxy to viewing all the concepts of a model.
These can then be compared to see why a specific layer has the most mutual information and how the join effects the resulting model.

\subsection{Jumping knowledge SGC}
\paragraph{Motivation}
\Sref{sec:concept-analysis} discusses the limited influence SGC has on node representations during message passing.
This is because an SGC model, of degree $k$, can only infer graph structure from the aggregated node representations of all neighbouring nodes within $k$ hops.
In comparison GCN is able to manipulate node representations between graph convolutions and can therefore learn to amplify or dampen differences between node representations during message passing which SGC cannot do.
This poses the potential the question of whether the low accuracy (seen in \Sref{sec:comp-acc}) and poor graph structure awareness (seen in \Sref{sec:comp-concept}) is due to the linearity or lack of influence.

To answer this I propose \emph{jump-SGC}(JSGC) which provides the classifier with node representations from each degree of the pre-computation.
This larger node representation is reduced before the classifier to maintain the same parameter size of the classifier.
This allows for JSGC to effectively manipulate the node representations though these manipulations do not have an impact on the successive future node representations unlike GCN.
This idea mimics \emph{jumping knowledge networks} (JKNs) proposed by \textit{Xu et al.}\cite{xu2018representation} hence the name ``jump''.

\paragraph{Prior work}
\textit{Xu et al.}\cite{xu2018representation} identify the drawbacks of node aggregation in accurately representing a nodes neighbourhood.
It specifically identifies the effect on graph structure awareness this has making the method ideal for SGC.
The motivation for JCNs is the node aggregation methods used resulted in neighbourhood influence similar to a random walk rather than a uniform influence.
As a solution they propose aggregating the the node representations after successive neighbourhood aggregation layers together.
Three main methods of aggregation are proposed but given the small size of the datasets the proposed concatenation method is best suited.

\paragraph{Implementation}
By concatenating successive neighbourhood aggregations and then reducing the dimensionality to a single node representation uniform influence can be achieved.
This is because detail present in the closer neighbourhoods can be combined with the wider awareness of the more receptive neighbourhoods.
Rather than missing larger structure awareness or missing detail JKNs allow for an analysis of both.

For JSGC this leads to two changes to the model and pre-computation.
During pre-computation successive applications of the normalised filter are concatenated together.
A linear layer is then added to the standard SGC to reduce this concatenated representation space to the standard representation space.
During this stage JSGC is able to infer more complex graph structure than SGC.
To combine this with the classifier a single non-linear rectified linear unit layer is introduced.
This non-linearity remains constant regardless of how the model scales and therefore the added potential benefits of the single non-linear layer is deemed negligible.

\section{Repository}

\dirtree{%
.1 \myfolder{black}{}.
.2   \myfolder{red}{README.md}.
.2   \myfolder{red}{requirements.txt}.
.2  \myfolder{blue}{activations.................................extracted activation space}.
.2  \myfolder{blue}{checkpoints..........................................model checkpoints}.
.2  \myfolder{blue}{data...............................................downloaded datasets}.
.2  \myfolder{blue}{output...............................model accuracy and concept output}.
.3  \myfolder{blue}{GCN-BA-Shapes.........................results from this combination}.
.3  \myfolder{blue}{...}.
.2  \myfolder{blue}{run}.
.3 \myfolder{green}{GCN-BA-Shapes.sh..............................experiment run script}.
.3 \myfolder{green}{...}.
.2  \myfolder{blue}{scripts}.
.3 \myfolder{green}{mk\_expr.sh..................................experiment build script}.
.3 \myfolder{green}{...}.
.2  \myfolder{blue}{src}.
.3   \myfolder{red}{eval.py..........................................concept evaluation}.
.3   \myfolder{red}{main.py.........................................ML pipeline control}.
.3   \myfolder{red}{optimise.py.........................................hyperopt script}.
.3   \myfolder{red}{sweep.py......................................hyperparameter sweeps}.
.3  \myfolder{blue}{configs...................................experiment configurations}.
.3  \myfolder{blue}{concepts}.
.4   \myfolder{red}{cluster.py.................................clustering algorithms}.
.4   \myfolder{red}{metrics.py....................completeness and purity algorithms}.
.4   \myfolder{red}{plotting.py..............................visualisation functions}.
.4  \myfolder{blue}{tests}.
.3  \myfolder{blue}{datasets}.
.4   \myfolder{red}{synthetic.py.............................synthetic dataset class}.
.4  \myfolder{blue}{tests}.
.3  \myfolder{blue}{loaders}.
.4   \myfolder{red}{utils.py...............................pre-computation functions}.
.4  \myfolder{blue}{tests}.
.3  \myfolder{blue}{models}.
.4   \myfolder{red}{gcn.py.................................................GCN model}.
.4   \myfolder{red}{sgc.py.................................................SGC model}.
.4   \myfolder{red}{activation\_classifier.py................decision tree classifier}.
.4  \myfolder{blue}{layers.............................................custom layers}.
.3  \myfolder{blue}{wrappers.................................pytorch lightning wrappers}.
}

The repository structure for my projecy.
The structure loosely follows ML project structures such as those provided by \texttt{cookiecutter} but is mainly driven by the requirements of \texttt{PyTorch Lighntning}.
Data folders are kept at the top level including folders for bash scripts allowing the scripts to access all the source code and folders necessary.
the \texttt{src} folder contains all the python code with top level python files such as \texttt{main.py} and \texttt{eval.py} acting as control code for the algorithms in the subfolders.
\hlc[blue!50]{blue} icons represent directories, \hlc[red!50]{red} icons represent files, and \hlc[green!50]{green} icons represent executables.
