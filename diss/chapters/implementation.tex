\chapter{Implementation}

\section{Datasets}
\label{sec:datasets-imp}
\Sref{sec:datasets-theory} presents a number of datasets that cover a range of different GNN training styles and motifs.
The transductive datasets discussed in \Sref{sec:RWD} are already packaged and available through \texttt{PyTorch Geometric}\cite{Fey/Lenssen/2019} as well as the Planetoid\cite{planetoid}\cite{citation} datasets.

The synthetic datasets discussed in \Sref{sec:synth} are proposed by \textit{Ying et al.}\cite{ying2019gnnexplainer}.
Though downloadable versions of these datasets are available from their github and \texttt{PyTorch Geometric} provide limited versions of a small subset I decide to implement them myself.
This allows for more rigorous testing of both the datasets and the SGC precomputation later described in \Sref{sec:testing-imp}.

Though the datasets are very simple they need to be compliant with \textit{PyTorch Geometric} to be used correctly during training.
\textit{PyTorch Geometric} provides an \texttt{InMemoryDataset} abstract class for datasets that are small enough to be generated and stored within RAM.

\error{Finish this!}

\section{Models}

\section{Testing}
\label{sec:testing-imp}

\section{Machine Learning Pipeline}

\subsection{Reproducibility}

\subsection{Experimentation}

\section{Concept Extraction and Evaluation}

\subsection{Extraction}

\subsection{Evaluation}

\subsection{Visualisation}

\section{Extensions}
\note{Include a short preamble.}
The evaluation of my extensions is presented in \Sref{sec:extension-eval}.

\subsection{SGC graph classification}
\error{Make sure this makes sense to flow.}
\paragraph{Motivation}
The datasets present in \textit{Magister et al.}\cite{magister2021gcexplainer} include 2 real world datasets that focus on graph classification.
As discussed in \Sref{sec:datasets} these graph classification datasets are also inductive rather than transductive which provides another test of the capabilities of SGC.
Furthermore, though \Sref{sec:comp-acc} suggests SGC performs poorly, this could be because of the synthetic nature of the datasets.

\paragraph{Prior work and implementation}
\textit{Wu et al.}\cite{wu2019simplifying} discuss graph classification for SGC suggesting that it can substitute GCN in a deep graph convolutional neural network as proposed by \note{name needed}\note{citation needed}.
However, \textit{Magister et al.} utilise pooling on the graph node representations after GCN to classify graphs. As this latter method is easier to implement and allows for a fairer comparison between SGC and GCN this approach is chosen.
The resulting model is identical to the standard SGC model with addition of a pooling layer before the classifier.

The only problem posed is to concept extraction as this is done on a node level as suggested by \textit{Magister et al.}.
This is overcome by broadcasting the graph label to each of the nodes.
This allows the graphs to be combined into a disconnected forest of graphs and clustering can be carried out on this forest.
Calculation of concept scores and the visualisation of concept therefore remains the same.

\subsection{SGC and GCN mixed model}
\error{Make sure this makes sense to flow.}
\paragraph{Motivation}
SGC is directly derived from GCN, the derivation provided in \Sref{sec:SGC}, using the same graph filter.
The benefit of SGC is to reduce the complexity of GCN and the cost of training by pre-computing the graph operation.
Though for the presented datasets in \Sref{sec:datasets} the improvement on cost and reduction in parameters is not significant larger datasets may create a larger benefit.
However, due to the low accuracy of SGC some aspects of GCN must required for high accuracy, therefore a mixture of both should yield a high accuracy model with pre-computation.
This combined model, \textit{SGC+GCN} (SGCN), maintains the same hyperparameters as the SGC and GCN models with the additional hyperparameter of whether a layer is SGC or GCN.

\paragraph{Implementation}
For simplicity and to utilise the power of pre-computation SGCN starts with SGC layers and then transitions to GCN layers.
For a fairer comparison the total number of layers must remain the same as the corresponding SGC and GCN layers.
This reduces the problem to finding where SGC and GCN agree the most in regards to their concepts.

This is achieved using mutual information between the two models by using the probability of a node appearing in a specific cluster from either model.
This gives a measure for the dependence of the models clusters and therefore how similar the models are.
However, this does not take into account random chance of two nodes appearing in the same cluster.
Therefore the mutual information is adjusted for this chance resulting in a number in the range $[0, 1]$ where $1$ is identical.
This is known as \emph{adjusted mutual information}(AMI).

To better visualise these results the dimensionality of the activations from the models are reduced using \emph{t-distributed stochastic neighbor embedding} (t-SNE)\note{citation needed} into 2 dimensions.
This clusters similar representations together and keeps different representations apart which acts as a visual proxy to viewing all the concepts of a model.
These can then be compared to see why a specific layer has the most mutual information and how the join effects the resulting model.

\subsection{Jumping knowledge SGC}
\error{Make sure this makes sense to flow.}
\paragraph{Motivation}
As discussed in \Sref{sec:concept-analysis} SGC does not have influence on the node representations during message passing.
SGC, of degree $k$, therefore has to infer graph structure from the aggregated node representations of all neighbouring nodes within $k$ hops.
Therefore the reason for the low accuracy and poor graph structure awareness may not be due to the linearity.
GCN is able to manipulate node representations between graph convolutions and can therefore further distinguish graph structure by potentially amplifying or damping differences between nodes.

For these reasons I propose \emph{jump-SGC}(JSGC) which provides the classifier with node representations from each degree of the pre-computation.
A fully-connected layer is added before the classifier to reduce the concatenated node representations into a single node representation.
This allows for JSGC to effectively manipulate the node representations though these manipulations do not have an impact on the application of the graph filter.
This idea mimics the \emph{jumping knowledge networks} (JCNs) proposed by \textit{Xu et al.}\cite{xu2018representation} hence the name ``jump''.

\paragraph{Prior work and Implementation}
\textit{Xu et al.}\cite{xu2018representation} identify the drawbacks of node aggregation in accurately representing a nodes neighbourhood.
It specifically identifies the effect on graph structure awareness this has making the method ideal for SGC.
The motivation for JCNs is the node aggregation methods used resulted in neighbourhood influence similar to a random walk rather than a uniform influence.
As a solution they propose aggregating the the node representations after successive neighbourhood aggregation layers together.
Three main methods of aggregation are proposed but given the small size of the datasets the proposed concatenation method is best suited.

By concatenating successive neighbourhood aggregations and then reducing the dimensionality to a single node representation uniform influence can be achieved.
This is because detail present in the closer neighbourhoods can be combined with the wider awareness of the more receptive neighbourhoods.
Rather than missing larger structure awareness or missing detail JCNs allow for an analysis of both.

For JSGC this leads to two changes to the model and pre-computation.
During pre-computation successive applications of the normalised filter are concatenated together.
A fully-connected layer is then added to the standard SGC to reduce this concatenated representation space to the standard representation space.
During this stage JSGC is able to infer more complex graph structure than SGC.
To combine this with the classifier a single non-linear rectified linear unit layer is introduced.
This non-linearity remains constant regardless of how the model scales and therefore the added potential benefits of the single non-linear layer is deemed negligible.

\section{Pytorch Geometric}

\section{Repository}
