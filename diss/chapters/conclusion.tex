\chapter{Conclusion}

\section{Summary}
This dissertation set out to analyse how linearising GNN architectures match the performance of their non-linear counterparts.
%It aimed to understand how graph structure can be better utilised by analysing these two approaches.
%This was achieved by comparing graph concepts from GCN and its linear counterpart, SGC.
As \Sref{sec:comp-acc} demonstrated the linear GNN, SGC, does not in fact match the performance of its non-linear counterpart.
\Sref{sec:comp-concept} highlights the poor graph structure awareness of SGC suggesting that non-linearity is key to inferring structure.
Extending SGC to graph classification also shows the same lack of graph structure awareness.

To combat these limitations SGC and GCN are combined to utilise the benefits of both approaches.
However, though this is successfull the use of SGC is very limited with the GCN layers providing the majority of the inference.

JSGC provides a novel extension to SGC with promising results highlighted in \Sref{sec:Jump-SGC} clearly demonstrating graph structure awareness.
JSGC provides SGC with intermediate node representations through techniques present in JKNs\cite{xu2018representation} allowing for better structural influences.
Importantly JSGC is negligibly non-linear demonstrating that graph structure awareness is possible within a linear architecture contrary to popular opinion.

\section{Lessons learned}
\note{New approach discussing lesson learnt \emph{about GNNs}.}
%The key skill learnt during the project was time management including careful planning allowing for unforeseen setbacks.
%Quick adaptation throughout was key as the majority of the project was research focused requiring exploration based on results.
%Maintaining good software practices and results loggin allowed for easier pivoting throughout the project.
%
%Saving all of the results that I had achieved throughout the project in a presentable format both helped quickly identify interesting results but also speed up the writing process.
%This is also true for keeping all the configuration files including those during experimentation as I was able to review past experiments when more knowledge was gained.
%Keeping notes on all the papers I read was also very useful however in future I would keep a better log of thoughts throughout the project.
%
%NN models are very sensitive to the hyperparameters used which can have a large impact on the apparent performance.
%The process of finding hyperparameters is very tedious as a large search space needs to be explored to verify that the chosen hyperparameters are optimal within that space.
%automating this process was very useful however I completed this process late in my project to verify the results that I had achieved are the best possible configuration.
%Though carrying out hyperparameter sweeps on all of my models did not effect the final analysis in future I will complete this step much earlier.

\section{Future work}
\paragraph{Other GNN architectures}
The results of SGC are surprising given the comparable performance demonstrated in \textit{Wu et al.}\cite{wu2019simplifying}.
There have been multiple other GNN architectures that have been proposed since the inception of GNNs.
Though these achieve high performance on a number of graph datasets this may not mean they have good graph structure awareness, as seen with SGC.
If these methods do demonstrate good graph structure awareness it is still important that how they reason is understood.

\paragraph{JSGC}
Graph structure awareness has been shown within a near linear model suggesting that the current approach to GNN architecture may be overly complex.
The capabilities and limitations of JSGC have not been explored fully in this dissertation but the results in \Sref{sec:Jump-SGC} are worthy of further consideration.
Though the accuracy of JSGC is not as high as GCN it is important to note that JSGC uses only 450 parameters compared to GCN's 1000+ on BA Shapes.

\paragraph{AMI mixed models}
To combine SGC and GCN the concepts that can be extracted are compared using AMI to determine where the two approaches are most similar.
As demonstrated in \Sref{sec:SGCN} in figures \ref{fig:SGCN-SGC-latent-space} and \ref{fig:SGCN-GCN-latent-space} the resulting model retains the influence of both models.
\textit{Shchur et al.}\cite{shchur2018pitfalls} and \textit{Purchase et al.}\cite{purchase2022revisiting} demonstrate the best performing GNN is highly dependent on training setup and dataset.
By combining the best GNN models in these different environments using the AMI approach it may be possible to overcome these limitations.
