\chapter{Conclusion}

\section{Summary}
This dissertation set out to analyse how linearising GNN architectures match the performance of their non-linear counterparts.
%It aimed to understand how graph structure can be better utilised by analysing these two approaches.
%This was achieved by comparing graph concepts from GCN and its linear counterpart, SGC.
As \Sref{sec:comp-acc} demonstrated the linear GNN, SGC, does not match the performance of its non-linear counterpart.
\Sref{sec:comp-concept} highlights the poor graph structure awareness of SGC suggesting that non-linearity is key to inferring structure.
Extending SGC to graph classification also shows the same lack of graph structure awareness.

To combat these limitations SGC and GCN are combined to utilise the benefits of both approaches.
However, though this is successful the use of SGC is very limited with the GCN layers providing the majority of the inference.

JSGC provides a novel extension to SGC with promising results highlighted in \Sref{sec:Jump-SGC} demonstrating graph structure awareness with an improvement of up to $50\%$ in accuracy compared to SGC.
JSGC provides SGC with intermediate node representations through the techniques present in JKNs~\cite{xu2018representation} allowing for influence and thus structural inference.
This demonstrates the performance of GNNs lies in manipulating and utilising the graph topology and from the non-linearity as is the case for standard NNs.
Importantly JSGC is negligibly non-linear demonstrating that graph structure awareness is possible within a linear architecture.

\section{Lessons learned}
My project aimed to understand the effect that linearising GNN architectures had on performance through graph concepts.
This effect depends as much on the dataset, the method of GNN explainability, and architecture as it does on the method of linearization.
Regardless, the project explored an area of this space and provided important insights.

Importantly, as shown by the proposed JSGC architecture, it is possible to achieve high graph structure awareness, achieving completeness scores of $0.943$ nearly reaching maximum completeness, whilst remaining negligibly non-linear.
This presents a new avenue of GNN architecture design outside of the standard deep learning approach and has utility in efficient graph structure inference.
But, as demonstrated by the poor performance of SGC, care must be taken in producing linear architectures to retain graph structure awareness.

The project did however demonstrate that non-linearity does provides higher accuracy when compared to linear architectures (GCN beats quasi-linear methods by at least $10\%$ and fully linear methods by $\approx 40\%$).
The results presented do not suggest that linear NNs are ideal but rather that graph structure awareness may be achieved through linear GNNs.
This distinction is highlighted by the poor performance of SGC and JSGC's inability to match the accuracy of GCN whilst achieving comparable graph structure awareness.
These drawbacks may potentially be solved by utilising more complex classifiers such as MLPs.

%The key skill learnt during the project was time management including careful planning allowing for unforeseen setbacks.
%Quick adaptation throughout was key as the majority of the project was research focused requiring exploration based on results.
%Maintaining good software practices and results loggin allowed for easier pivoting throughout the project.
%
%Saving all of the results that I had achieved throughout the project in a presentable format both helped quickly identify interesting results but also speed up the writing process.
%This is also true for keeping all the configuration files including those during experimentation as I was able to review past experiments when more knowledge was gained.
%Keeping notes on all the papers I read was also very useful however in future I would keep a better log of thoughts throughout the project.
%
%NN models are very sensitive to the hyperparameters used which can have a large impact on the apparent performance.
%The process of finding hyperparameters is very tedious as a large search space needs to be explored to verify that the chosen hyperparameters are optimal within that space.
%automating this process was very useful however I completed this process late in my project to verify the results that I had achieved are the best possible configuration.
%Though carrying out hyperparameter sweeps on all of my models did not effect the final analysis in future I will complete this step much earlier.

\section{Future work}
\paragraph{JSGC}
Graph structure awareness has been shown within a quasi-linear model suggesting that the current approach to GNN architecture may be overly complex.
The capabilities and limitations of JSGC have not been explored fully in this dissertation but the results in \Sref{sec:Jump-SGC} are worthy of further consideration.
%Though the accuracy of JSGC is not as high as GCN it is important to note that JSGC uses only 350 parameters compared to GCN's $>$1000 on BA Shapes.
Though the performance of JSGC is low (GCN beats JSGC by at least $16\%$) it may be best utilised in conjunction with more sophisticated classifiers separating graph structure inference from node label inference.

\paragraph{AMI mixed models}
To combine SGC and GCN the extracted concepts are compared using AMI to determine where the two approaches are similar.
As demonstrated in \Sref{sec:SGCN-eval} in figure \ref{fig:SGCN-latent-spaces} the resulting model retains the influence of both models.
%\textit{Shchur et al.}~\cite{shchur2018pitfalls} and \textit{Purchase et al.}~\cite{purchase2022revisiting} demonstrate the best performing GNN is highly dependent on training setup and dataset.
By combining the best-performing GNN models in different environments using the AMI approach it may be possible to overcome the individual GNN limitations.

\paragraph{Other GNN architectures}
Multiple GNN architectures have been proposed which achieve high accuracy on many benchmark datasets.
However, there have been limited attempts to understand how the architectures achieve high performance outside of theoretical proofs.
Applying explainability tools, such as \textit{GCExplainer}~\cite{magister2021gcexplainer}, will provide further insight and present possible areas of improvement.
